{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541b540e-4a5d-4194-8f3d-48512198d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e10eebe-40f5-467c-b9e5-d6150ca62154",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_excel(r'C:\\Users\\user\\Desktop\\Varsha\\time balance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c833535a-e1b0-42d7-a271-00b877216dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31617b9-ca1a-4ade-92fc-6e8ecb85ae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291491, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c63ea4-a84d-4a17-aed0-82e1385bc076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major code stored  as  workover/drilling         0\n",
      "Cde                                              0\n",
      "Service hours type                               0\n",
      "Minor Repair / Tripping / Complication      201202\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Major code stored  as  workover/drilling</th>\n",
       "      <th>Cde</th>\n",
       "      <th>Service hours type</th>\n",
       "      <th>Minor Repair / Tripping / Complication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A y POOH TO SURF. B/OFF &amp; L/DN DIR ASSY.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>POOH.</td>\n",
       "      <td>6A02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A &amp; R/I SAME BHA &amp; TAGGED BOTTOM.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A &amp; P/O 61 STDS + 02 SGLS OF 5” D/P.</td>\n",
       "      <td>6A02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A &amp; P/O TO 1650M ( INSIDE CSG SHOE)</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291479</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>W/TRIP.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291483</th>\n",
       "      <td>PTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>WELL READY FOR PACKER SETTING JOB.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291486</th>\n",
       "      <td>PTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>POOH</td>\n",
       "      <td>6A02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291487</th>\n",
       "      <td>PTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>R/I WITH TCP GUN ASSY IS IN PROGRESS</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291490</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>CRACKED POOH FOR CL</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90289 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Major code stored  as  workover/drilling Cde  \\\n",
       "12                                         DTRP  6A   \n",
       "20                                         DTRP  6A   \n",
       "23                                         DTRP  6A   \n",
       "25                                         DTRP  6A   \n",
       "28                                         DTRP  6A   \n",
       "...                                         ...  ..   \n",
       "291479                                     DTRP  6A   \n",
       "291483                                     PTRP  6A   \n",
       "291486                                     PTRP  6A   \n",
       "291487                                     PTRP  6A   \n",
       "291490                                     DTRP  6A   \n",
       "\n",
       "                                Service hours type  \\\n",
       "12      M/A y POOH TO SURF. B/OFF & L/DN DIR ASSY.   \n",
       "20                                           POOH.   \n",
       "23             M/A & R/I SAME BHA & TAGGED BOTTOM.   \n",
       "25          M/A & P/O 61 STDS + 02 SGLS OF 5” D/P.   \n",
       "28           M/A & P/O TO 1650M ( INSIDE CSG SHOE)   \n",
       "...                                            ...   \n",
       "291479                                     W/TRIP.   \n",
       "291483          WELL READY FOR PACKER SETTING JOB.   \n",
       "291486                                        POOH   \n",
       "291487        R/I WITH TCP GUN ASSY IS IN PROGRESS   \n",
       "291490                         CRACKED POOH FOR CL   \n",
       "\n",
       "       Minor Repair / Tripping / Complication  \n",
       "12                                       6A01  \n",
       "20                                       6A02  \n",
       "23                                       6A01  \n",
       "25                                       6A02  \n",
       "28                                       6A01  \n",
       "...                                       ...  \n",
       "291479                                   6A01  \n",
       "291483                                   6A01  \n",
       "291486                                   6A02  \n",
       "291487                                   6A01  \n",
       "291490                                   6A01  \n",
       "\n",
       "[90289 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9bfe00-0aee-4243-bd60-e7f530f989ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "   if isinstance(text,str):\n",
    "      text = text.lower()  # Convert to lowercase\n",
    "      text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "      text = re.sub(r'\\s+', ' ', text).strip() #Remove extra-spaces\n",
    "      return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply text preprocessing\n",
    "df['Service hours type_lowercase'] = df['Service hours type'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7997b377-cc68-4bc1-900c-cb8b195e27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Service hours type_lowercase'] = df['Service hours type_lowercase'].apply(lambda x: str(x) if isinstance(x, str) else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f084ec0-f4ae-4dc2-ad62-c49f1c9707a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93feab61-8e03-4da0-9519-3962584e998c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Major code stored  as  workover/drilling</th>\n",
       "      <th>Cde</th>\n",
       "      <th>Service hours type</th>\n",
       "      <th>Minor Repair / Tripping / Complication</th>\n",
       "      <th>Service hours type_lowercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RDRP</td>\n",
       "      <td>1A</td>\n",
       "      <td>RIG CARRIER MOVED TO NEW SITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rig carrier moved to new site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Major code stored  as  workover/drilling  Cde  \\\n",
       "0                                     RDRP   1A   \n",
       "1                                      OUC  24J   \n",
       "2                                      OUC  24J   \n",
       "3                                      OUC  24J   \n",
       "4                                      OUC  24J   \n",
       "\n",
       "                                  Service hours type  \\\n",
       "0                      RIG CARRIER MOVED TO NEW SITE   \n",
       "1  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "2  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "3  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "4  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "\n",
       "  Minor Repair / Tripping / Complication  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "                       Service hours type_lowercase  \n",
       "0                     rig carrier moved to new site  \n",
       "1  dws under major repair waitg for comp of civ job  \n",
       "2  dws under major repair waitg for comp of civ job  \n",
       "3  dws under major repair waitg for comp of civ job  \n",
       "4  dws under major repair waitg for comp of civ job  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d5e617-4fa7-4f20-9fbc-ed01b3d9469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_rare_samples(df, column_name, threshold=10):\n",
    "    # Get the frequency of each unique value in the specified column\n",
    "    value_counts = df[column_name].value_counts()\n",
    "\n",
    "    # Filter the DataFrame to retain only the rows where the count is greater than or equal to the threshold\n",
    "    fdf = df[df[column_name].isin(value_counts[value_counts >= threshold].index)]\n",
    "\n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73dbc41-6c6f-4848-b699-b7632c51907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = discard_rare_samples(df,'Cde',threshold =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd5ff5e-9013-4092-a47f-ceeb05cb67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c1eb3c-8118-4126-a62c-f97c80bdfd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291489, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cb984af-7d6b-429b-a9e2-59459d670347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Major code stored  as  workover/drilling', 'Cde', 'Service hours type',\n",
       "       'Minor Repair / Tripping / Complication',\n",
       "       'Service hours type_lowercase'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77b0bf5a-6909-4a97-a329-d754bca9baf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cde\n",
       "6A     67930\n",
       "5A     38754\n",
       "2A     22671\n",
       "23A    13612\n",
       "11A    13301\n",
       "       ...  \n",
       "23I       76\n",
       "7B        70\n",
       "11B       56\n",
       "24K       45\n",
       "21J       10\n",
       "Name: count, Length: 73, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf['Cde'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e860c04-10a3-477c-b9e8-c803fc29db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f515c0-7233-4a49-9660-9410276fbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f2d15fc-9f13-473b-ab13-1442063a892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.loc[:, 'label_encoded'] = le.fit_transform(fdf['Cde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44780ca2-8587-4900-9b41-bee98a07600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Major code stored  as  workover/drilling', 'Cde', 'Service hours type',\n",
       "       'Minor Repair / Tripping / Complication',\n",
       "       'Service hours type_lowercase', 'label_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90b4efa8-87fa-4550-b0d7-cc908c0b322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = fdf.drop(columns=['label_encoded'])  # Features\n",
    "y = fdf['label_encoded']  # Target\n",
    "\n",
    "# Perform stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,  # 30% test set\n",
    "    stratify=y,     # Ensure class distribution is preserved\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine features and target back into DataFrames\n",
    "train_df = X_train.copy()\n",
    "train_df['label_encoded'] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df['label_encoded'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "253878c5-0235-4f11-8a41-d03dc1575932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts have been saved to 'class_counts.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count the number of samples per class in the training set\n",
    "train_counts = train_df['label_encoded'].value_counts().sort_index()\n",
    "\n",
    "# Count the number of samples per class in the testing set\n",
    "test_counts = test_df['label_encoded'].value_counts().sort_index()\n",
    "\n",
    "# Create a DataFrame to store the counts\n",
    "class_counts_df = pd.DataFrame({\n",
    "    'Class': train_counts.index,\n",
    "    'Train_Count': train_counts.values,\n",
    "    'Test_Count': test_counts.reindex(train_counts.index, fill_value=0).values\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "class_counts_df.to_excel(r'C:\\Users\\user\\Desktop\\Varsha/class_counts.xlsx', index=False)\n",
    "\n",
    "\n",
    "print(\"Class counts have been saved to 'class_counts.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "322224f9-4aaf-49d2-8973-fe63816bacce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_encoded\n",
      "65    0.233045\n",
      "64    0.132953\n",
      "59    0.077778\n",
      "41    0.046696\n",
      "1     0.045633\n",
      "        ...   \n",
      "49    0.000260\n",
      "67    0.000240\n",
      "2     0.000191\n",
      "58    0.000157\n",
      "29    0.000034\n",
      "Name: proportion, Length: 73, dtype: float64\n",
      "label_encoded\n",
      "65    0.233044\n",
      "64    0.132949\n",
      "59    0.077773\n",
      "41    0.046703\n",
      "1     0.045628\n",
      "        ...   \n",
      "49    0.000263\n",
      "67    0.000240\n",
      "2     0.000194\n",
      "58    0.000149\n",
      "29    0.000034\n",
      "Name: proportion, Length: 73, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify class distribution\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfac22fe-ecd8-42df-a30e-d946e834172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in d:\\hehe\\envs\\pytorch\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console contrib dejavu events execute kernel kernelspec\n",
      "lab labextension labhub migrate nbconvert nbextensions_configurator notebook\n",
      "qtconsole run script server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --upgrade\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f65858dc-baaf-498e-9ce9-2ea3e95a17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter-contrib-nbextensions in d:\\hehe\\envs\\pytorch\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: ipython-genutils in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (0.2.0)\n",
      "Requirement already satisfied: jupyter-contrib-core>=0.3.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (0.4.2)\n",
      "Requirement already satisfied: jupyter-core in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (5.7.2)\n",
      "Requirement already satisfied: jupyter-highlight-selected-word>=0.1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (0.2.0)\n",
      "Requirement already satisfied: jupyter-nbextensions-configurator>=0.4.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (0.6.4)\n",
      "Requirement already satisfied: nbconvert>=6.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (7.10.0)\n",
      "Requirement already satisfied: notebook>=6.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (7.0.8)\n",
      "Requirement already satisfied: tornado in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=4.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (5.14.3)\n",
      "Requirement already satisfied: lxml in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (5.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-core>=0.3.3->jupyter-contrib-nbextensions) (72.1.0)\n",
      "Requirement already satisfied: jupyter-server in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (2.14.1)\n",
      "Requirement already satisfied: pyyaml in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (3.1.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (5.9.2)\n",
      "Requirement already satisfied: packaging in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (24.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (1.2.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-core->jupyter-contrib-nbextensions) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-core->jupyter-contrib-nbextensions) (305.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from notebook>=6.0->jupyter-contrib-nbextensions) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<4.1,>=4.0.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from notebook>=6.0->jupyter-contrib-nbextensions) (4.0.11)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from notebook>=6.0->jupyter-contrib-nbextensions) (0.2.3)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter-contrib-nbextensions) (1.16.0)\n",
      "Requirement already satisfied: webencodings in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter-contrib-nbextensions) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (21.3.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (8.6.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (0.4.4)\n",
      "Requirement already satisfied: overrides>=5.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (2.0.10)\n",
      "Requirement already satisfied: pyzmq>=24 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (6.28.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbformat>=5.7->nbconvert>=6.0->jupyter-contrib-nbextensions) (2.16.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.0->jupyter-contrib-nbextensions) (2.5)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2024.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (2.9.0.post0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2024.8.30)\n",
      "Requirement already satisfied: comm>=0.1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (8.25.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (1.6.0)\n",
      "Requirement already satisfied: psutil in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (5.9.0)\n",
      "Requirement already satisfied: decorator in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (3.0.43)\n",
      "Requirement already satisfied: stack-data in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.0)\n",
      "Requirement already satisfied: colorama in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.4.6)\n",
      "Requirement already satisfied: fqdn in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.5.1)\n",
      "Requirement already satisfied: isoduration in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (3.0.0)\n",
      "Requirement already satisfied: uri-template in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.5)\n",
      "Requirement already satisfied: arrow>=0.15.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter-nbextensions-configurator>=0.4.0->jupyter-contrib-nbextensions) (1.2.3)\n",
      "Requirement already satisfied: executing in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter-contrib-nbextensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1658a548-409c-43fb-86f2-59525b129a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\nbextensions.py\", line 6, in <module>\n",
      "    from notebook.extensions import BaseExtensionApp\n",
      "ModuleNotFoundError: No module named 'notebook.extensions'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\nbextensions.py\", line 10, in <module>\n",
      "    from notebook.nbextensions import BaseNBExtensionApp\n",
      "ModuleNotFoundError: No module named 'notebook.nbextensions'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Scripts\\jupyter-contrib.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1073, in launch_instance\n",
      "    app = cls.instance(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\traitlets\\config\\configurable.py\", line 583, in instance\n",
      "    inst = cls(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\application.py\", line 27, in __init__\n",
      "    self._refresh_subcommands()\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\application.py\", line 43, in _refresh_subcommands\n",
      "    get_subcommands_dict = entrypoint.load()\n",
      "                           ^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2771, in load\n",
      "    return self.resolve()\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2777, in resolve\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_nbextensions\\application.py\", line 7, in <module>\n",
      "    from jupyter_contrib_core.notebook_compat.nbextensions import ArgumentConflict\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\nbextensions.py\", line 12, in <module>\n",
      "    from ._compat.nbextensions import BaseNBExtensionApp\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\_compat\\nbextensions.py\", line 35, in <module>\n",
      "    from notebook.nbextensions import (\n",
      "ModuleNotFoundError: No module named 'notebook.nbextensions'\n"
     ]
    }
   ],
   "source": [
    "!jupyter contrib nbextension install --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b97fa97-f833-4676-bdd8-0b3f36709fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2326, 2847, 2828, 1035, 2896, 18382, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokens = tokenizer('Service hours type_lowercase')\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2121992f-4c0e-4c11-9031-58e7eee950cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training data\n",
    "train_encodings = tokenizer(list(train_df['Service hours type_lowercase']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize the testing data\n",
    "test_encodings = tokenizer(list(test_df['Service hours type_lowercase']), truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24f19d44-33e2-43e5-9a3a-37edc5c7735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = CustomDataset(train_encodings, list(train_df['label_encoded']))\n",
    "test_dataset = CustomDataset(test_encodings, list(test_df['label_encoded']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d09cc42f-4c8e-45f8-b3ce-fe5edd7180dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Number of labels in your classification task\n",
    "num_labels = len(train_df['label_encoded'].unique())\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5231317-b339-403f-a696-12a2d09b0c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\",\n",
       "    \"15\": \"LABEL_15\",\n",
       "    \"16\": \"LABEL_16\",\n",
       "    \"17\": \"LABEL_17\",\n",
       "    \"18\": \"LABEL_18\",\n",
       "    \"19\": \"LABEL_19\",\n",
       "    \"20\": \"LABEL_20\",\n",
       "    \"21\": \"LABEL_21\",\n",
       "    \"22\": \"LABEL_22\",\n",
       "    \"23\": \"LABEL_23\",\n",
       "    \"24\": \"LABEL_24\",\n",
       "    \"25\": \"LABEL_25\",\n",
       "    \"26\": \"LABEL_26\",\n",
       "    \"27\": \"LABEL_27\",\n",
       "    \"28\": \"LABEL_28\",\n",
       "    \"29\": \"LABEL_29\",\n",
       "    \"30\": \"LABEL_30\",\n",
       "    \"31\": \"LABEL_31\",\n",
       "    \"32\": \"LABEL_32\",\n",
       "    \"33\": \"LABEL_33\",\n",
       "    \"34\": \"LABEL_34\",\n",
       "    \"35\": \"LABEL_35\",\n",
       "    \"36\": \"LABEL_36\",\n",
       "    \"37\": \"LABEL_37\",\n",
       "    \"38\": \"LABEL_38\",\n",
       "    \"39\": \"LABEL_39\",\n",
       "    \"40\": \"LABEL_40\",\n",
       "    \"41\": \"LABEL_41\",\n",
       "    \"42\": \"LABEL_42\",\n",
       "    \"43\": \"LABEL_43\",\n",
       "    \"44\": \"LABEL_44\",\n",
       "    \"45\": \"LABEL_45\",\n",
       "    \"46\": \"LABEL_46\",\n",
       "    \"47\": \"LABEL_47\",\n",
       "    \"48\": \"LABEL_48\",\n",
       "    \"49\": \"LABEL_49\",\n",
       "    \"50\": \"LABEL_50\",\n",
       "    \"51\": \"LABEL_51\",\n",
       "    \"52\": \"LABEL_52\",\n",
       "    \"53\": \"LABEL_53\",\n",
       "    \"54\": \"LABEL_54\",\n",
       "    \"55\": \"LABEL_55\",\n",
       "    \"56\": \"LABEL_56\",\n",
       "    \"57\": \"LABEL_57\",\n",
       "    \"58\": \"LABEL_58\",\n",
       "    \"59\": \"LABEL_59\",\n",
       "    \"60\": \"LABEL_60\",\n",
       "    \"61\": \"LABEL_61\",\n",
       "    \"62\": \"LABEL_62\",\n",
       "    \"63\": \"LABEL_63\",\n",
       "    \"64\": \"LABEL_64\",\n",
       "    \"65\": \"LABEL_65\",\n",
       "    \"66\": \"LABEL_66\",\n",
       "    \"67\": \"LABEL_67\",\n",
       "    \"68\": \"LABEL_68\",\n",
       "    \"69\": \"LABEL_69\",\n",
       "    \"70\": \"LABEL_70\",\n",
       "    \"71\": \"LABEL_71\",\n",
       "    \"72\": \"LABEL_72\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_15\": 15,\n",
       "    \"LABEL_16\": 16,\n",
       "    \"LABEL_17\": 17,\n",
       "    \"LABEL_18\": 18,\n",
       "    \"LABEL_19\": 19,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_20\": 20,\n",
       "    \"LABEL_21\": 21,\n",
       "    \"LABEL_22\": 22,\n",
       "    \"LABEL_23\": 23,\n",
       "    \"LABEL_24\": 24,\n",
       "    \"LABEL_25\": 25,\n",
       "    \"LABEL_26\": 26,\n",
       "    \"LABEL_27\": 27,\n",
       "    \"LABEL_28\": 28,\n",
       "    \"LABEL_29\": 29,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_30\": 30,\n",
       "    \"LABEL_31\": 31,\n",
       "    \"LABEL_32\": 32,\n",
       "    \"LABEL_33\": 33,\n",
       "    \"LABEL_34\": 34,\n",
       "    \"LABEL_35\": 35,\n",
       "    \"LABEL_36\": 36,\n",
       "    \"LABEL_37\": 37,\n",
       "    \"LABEL_38\": 38,\n",
       "    \"LABEL_39\": 39,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_40\": 40,\n",
       "    \"LABEL_41\": 41,\n",
       "    \"LABEL_42\": 42,\n",
       "    \"LABEL_43\": 43,\n",
       "    \"LABEL_44\": 44,\n",
       "    \"LABEL_45\": 45,\n",
       "    \"LABEL_46\": 46,\n",
       "    \"LABEL_47\": 47,\n",
       "    \"LABEL_48\": 48,\n",
       "    \"LABEL_49\": 49,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_50\": 50,\n",
       "    \"LABEL_51\": 51,\n",
       "    \"LABEL_52\": 52,\n",
       "    \"LABEL_53\": 53,\n",
       "    \"LABEL_54\": 54,\n",
       "    \"LABEL_55\": 55,\n",
       "    \"LABEL_56\": 56,\n",
       "    \"LABEL_57\": 57,\n",
       "    \"LABEL_58\": 58,\n",
       "    \"LABEL_59\": 59,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_60\": 60,\n",
       "    \"LABEL_61\": 61,\n",
       "    \"LABEL_62\": 62,\n",
       "    \"LABEL_63\": 63,\n",
       "    \"LABEL_64\": 64,\n",
       "    \"LABEL_65\": 65,\n",
       "    \"LABEL_66\": 66,\n",
       "    \"LABEL_67\": 67,\n",
       "    \"LABEL_68\": 68,\n",
       "    \"LABEL_69\": 69,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_70\": 70,\n",
       "    \"LABEL_71\": 71,\n",
       "    \"LABEL_72\": 72,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8ac12e3-ffbb-4239-b07a-b8eb1f748851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cb675cd-13eb-486d-95ad-6c1c496bf27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=30,              # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    evaluation_strategy='epoch',     # Evaluate after every epoch\n",
    "    save_strategy='epoch',           # Save model checkpoints after every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e918455a-f74b-4e3c-b908-8d8a5bd739cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=test_dataset,           # Evaluation dataset\n",
    "    compute_metrics=compute_metrics     # Compute metrics function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a50335c-cc68-41fa-8aaf-51e8cccacb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='191310' max='191310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [191310/191310 7:34:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>0.731314</td>\n",
       "      <td>0.822876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>0.672967</td>\n",
       "      <td>0.834231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.684205</td>\n",
       "      <td>0.836587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.699857</td>\n",
       "      <td>0.837696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.285200</td>\n",
       "      <td>0.741741</td>\n",
       "      <td>0.836770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.788610</td>\n",
       "      <td>0.836415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.837215</td>\n",
       "      <td>0.832573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.922038</td>\n",
       "      <td>0.830457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.979742</td>\n",
       "      <td>0.830652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>1.092033</td>\n",
       "      <td>0.831898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>1.150642</td>\n",
       "      <td>0.829977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>1.188278</td>\n",
       "      <td>0.831738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>1.239272</td>\n",
       "      <td>0.829314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>1.336960</td>\n",
       "      <td>0.827461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>1.417529</td>\n",
       "      <td>0.831143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>1.486287</td>\n",
       "      <td>0.832561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>1.473215</td>\n",
       "      <td>0.832584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>1.554044</td>\n",
       "      <td>0.831544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>1.555693</td>\n",
       "      <td>0.832973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>1.598608</td>\n",
       "      <td>0.830469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>1.713115</td>\n",
       "      <td>0.833922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>1.712617</td>\n",
       "      <td>0.833602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>1.719481</td>\n",
       "      <td>0.836072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>1.757919</td>\n",
       "      <td>0.834185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>1.775536</td>\n",
       "      <td>0.832870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>1.825237</td>\n",
       "      <td>0.834871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>1.830679</td>\n",
       "      <td>0.835569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>1.852478</td>\n",
       "      <td>0.835249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>1.877466</td>\n",
       "      <td>0.834288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>1.903971</td>\n",
       "      <td>0.834974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=191310, training_loss=0.20570610128045175, metrics={'train_runtime': 27282.7322, 'train_samples_per_second': 224.364, 'train_steps_per_second': 7.012, 'total_flos': 4.59861890413223e+16, 'train_loss': 0.20570610128045175, 'epoch': 30.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed27afd5-2152-49a1-a4fb-301569074eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=73, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "save_directory = r'C:\\Users\\user\\Desktop\\Varsha\\model'\n",
    "\n",
    "# Load the model and tokenizer from the directory\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6675c51-ef0d-4554-8b01-e94e383b900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(list(test_df['Service hours type_lowercase']), truncation=True, padding=True, max_length=128, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5242d889-b44e-42d9-856c-241b8dcc6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = test_encodings['input_ids'].to(device)\n",
    "attention_mask = test_encodings['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e02a5c80-14f9-4394-9bf6-1ca9f329408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc381761-82d8-40a4-9cc4-51a029ed9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dcbd264-426e-4a3f-939a-fa08ca91d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "test_dataset = TensorDataset(input_ids, attention_mask)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71d89f7c-2a8e-4dc2-ac74-d1b3e0dcfe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8370\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "true_labels = list(test_df['label_encoded'])  # Extract true labels\n",
    "\n",
    "# Perform inference in batches\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch_input_ids, batch_attention_mask = batch\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert predictions list to a NumPy array\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(true_labels, all_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f21be3ab-847f-493c-929c-b48b38c8c81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8370\n",
      "       Class  Accuracy\n",
      "0    Class 0  0.778234\n",
      "1    Class 1  0.897744\n",
      "2    Class 2  0.235294\n",
      "3    Class 3  0.843330\n",
      "4    Class 4  0.717151\n",
      "..       ...       ...\n",
      "68  Class 68  0.539715\n",
      "69  Class 69  0.834397\n",
      "70  Class 70  0.723554\n",
      "71  Class 71  0.490000\n",
      "72  Class 72  0.963432\n",
      "\n",
      "[73 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Compute accuracy and class-wise accuracies\n",
    "accuracy = accuracy_score(true_labels, all_predictions)\n",
    "cm = confusion_matrix(true_labels, all_predictions)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "class_names = [f'Class {i}' for i in range(len(class_accuracies))]\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Accuracy': class_accuracies\n",
    "})\n",
    "\n",
    "# Print the accuracy and class-wise accuracy\n",
    "print(f'Overall Accuracy: {accuracy:.4f}')\n",
    "print(accuracy_df)\n",
    "\n",
    "# Save class-wise accuracies to an Excel file\n",
    "excel_path = r'C:\\Users\\user\\Desktop\\Varsha\\class_wise_accuracy.xlsx'\n",
    "accuracy_df.to_excel(excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01556b01-9343-45ee-bb57-721f8908559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes with less than 50% accuracy: ['Class 2', 'Class 11', 'Class 17', 'Class 18', 'Class 20', 'Class 29', 'Class 34', 'Class 36', 'Class 37', 'Class 38', 'Class 39', 'Class 40', 'Class 43', 'Class 44', 'Class 45', 'Class 46', 'Class 49', 'Class 60', 'Class 67', 'Class 71']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11568\\3402043413.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_samples['Class'] = class_name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All low accuracy samples saved to C:\\Users\\user\\Desktop\\Varsha\\class_wise_accuracy.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Excel file\n",
    "excel_path = r'C:\\Users\\user\\Desktop\\Varsha\\class_wise_accuracy.xlsx'\n",
    "\n",
    "# Save the class-wise accuracies to an Excel file\n",
    "accuracy_df.to_excel(excel_path, index=False)\n",
    "\n",
    "# Identify classes with less than 50% accuracy\n",
    "low_accuracy_classes = accuracy_df[accuracy_df['Accuracy'] < 0.50]['Class'].tolist()\n",
    "\n",
    "print(f'Classes with less than 50% accuracy: {low_accuracy_classes}')\n",
    "\n",
    "# Retrieve and prepare samples for classes with less than 50% accuracy\n",
    "original_column = 'Service hours type'\n",
    "pre_processed_column = 'Service hours type_lowercase'\n",
    "\n",
    "low_accuracy_samples = test_df[test_df['label_encoded'].isin([int(c.split(' ')[-1]) for c in low_accuracy_classes])]\n",
    "\n",
    "# Prepare a DataFrame to save all samples with less than 50% accuracy\n",
    "samples_data = []\n",
    "\n",
    "for label in low_accuracy_samples['label_encoded'].unique():\n",
    "    class_samples = low_accuracy_samples[low_accuracy_samples['label_encoded'] == label]\n",
    "    class_name = f'Class {label}'\n",
    "    \n",
    "    # Add a new column to indicate the class\n",
    "    class_samples['Class'] = class_name\n",
    "    \n",
    "    # Append the samples to the list\n",
    "    samples_data.append(class_samples[[original_column, pre_processed_column, 'Class']])\n",
    "\n",
    "# Combine all class samples into a single DataFrame\n",
    "all_samples_df = pd.concat(samples_data, ignore_index=True)\n",
    "\n",
    "# Save the samples DataFrame to the same Excel file but in a new sheet\n",
    "with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl') as writer:\n",
    "    all_samples_df.to_excel(writer, sheet_name='Low_Accuracy_Samples', index=False)\n",
    "\n",
    "print(f'All low accuracy samples saved to {excel_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "168d8a83-3783-418e-b4e5-a25244e00897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to C:\\Users\\user\\Desktop\\Varsha\\class_wise_accuracy.xlsx\n"
     ]
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "# Save the confusion matrix DataFrame to the same Excel file but in a new sheet\n",
    "with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl') as writer:\n",
    "    cm_df.to_excel(writer, sheet_name='Confusion_Matrix')\n",
    "\n",
    "print(f'Confusion matrix saved to {excel_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8459ff-1f8a-486c-9587-6be100e03515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
