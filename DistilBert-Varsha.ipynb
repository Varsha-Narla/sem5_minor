{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541b540e-4a5d-4194-8f3d-48512198d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e10eebe-40f5-467c-b9e5-d6150ca62154",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_excel(r'C:\\Users\\user\\Desktop\\time balance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c833535a-e1b0-42d7-a271-00b877216dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c63ea4-a84d-4a17-aed0-82e1385bc076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major code stored  as  workover/drilling         0\n",
      "Cde                                              0\n",
      "Service hours type                               0\n",
      "Minor Repair / Tripping / Complication      201202\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Major code stored  as  workover/drilling</th>\n",
       "      <th>Cde</th>\n",
       "      <th>Service hours type</th>\n",
       "      <th>Minor Repair / Tripping / Complication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A y POOH TO SURF. B/OFF &amp; L/DN DIR ASSY.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>POOH.</td>\n",
       "      <td>6A02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A &amp; R/I SAME BHA &amp; TAGGED BOTTOM.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A &amp; P/O 61 STDS + 02 SGLS OF 5” D/P.</td>\n",
       "      <td>6A02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>M/A &amp; P/O TO 1650M ( INSIDE CSG SHOE)</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291479</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>W/TRIP.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291483</th>\n",
       "      <td>PTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>WELL READY FOR PACKER SETTING JOB.</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291486</th>\n",
       "      <td>PTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>POOH</td>\n",
       "      <td>6A02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291487</th>\n",
       "      <td>PTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>R/I WITH TCP GUN ASSY IS IN PROGRESS</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291490</th>\n",
       "      <td>DTRP</td>\n",
       "      <td>6A</td>\n",
       "      <td>CRACKED POOH FOR CL</td>\n",
       "      <td>6A01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90289 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Major code stored  as  workover/drilling Cde  \\\n",
       "12                                         DTRP  6A   \n",
       "20                                         DTRP  6A   \n",
       "23                                         DTRP  6A   \n",
       "25                                         DTRP  6A   \n",
       "28                                         DTRP  6A   \n",
       "...                                         ...  ..   \n",
       "291479                                     DTRP  6A   \n",
       "291483                                     PTRP  6A   \n",
       "291486                                     PTRP  6A   \n",
       "291487                                     PTRP  6A   \n",
       "291490                                     DTRP  6A   \n",
       "\n",
       "                                Service hours type  \\\n",
       "12      M/A y POOH TO SURF. B/OFF & L/DN DIR ASSY.   \n",
       "20                                           POOH.   \n",
       "23             M/A & R/I SAME BHA & TAGGED BOTTOM.   \n",
       "25          M/A & P/O 61 STDS + 02 SGLS OF 5” D/P.   \n",
       "28           M/A & P/O TO 1650M ( INSIDE CSG SHOE)   \n",
       "...                                            ...   \n",
       "291479                                     W/TRIP.   \n",
       "291483          WELL READY FOR PACKER SETTING JOB.   \n",
       "291486                                        POOH   \n",
       "291487        R/I WITH TCP GUN ASSY IS IN PROGRESS   \n",
       "291490                         CRACKED POOH FOR CL   \n",
       "\n",
       "       Minor Repair / Tripping / Complication  \n",
       "12                                       6A01  \n",
       "20                                       6A02  \n",
       "23                                       6A01  \n",
       "25                                       6A02  \n",
       "28                                       6A01  \n",
       "...                                       ...  \n",
       "291479                                   6A01  \n",
       "291483                                   6A01  \n",
       "291486                                   6A02  \n",
       "291487                                   6A01  \n",
       "291490                                   6A01  \n",
       "\n",
       "[90289 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9bfe00-0aee-4243-bd60-e7f530f989ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "   if isinstance(text,str):\n",
    "      text = text.lower()  # Convert to lowercase\n",
    "      text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "      text = re.sub(r'\\s+', ' ', text).strip() #Remove extra-spaces\n",
    "      return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply text preprocessing\n",
    "df['Service hours type_lowercase'] = df['Service hours type'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7997b377-cc68-4bc1-900c-cb8b195e27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Service hours type_lowercase'] = df['Service hours type_lowercase'].apply(lambda x: str(x) if isinstance(x, str) else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f084ec0-f4ae-4dc2-ad62-c49f1c9707a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93feab61-8e03-4da0-9519-3962584e998c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Major code stored  as  workover/drilling</th>\n",
       "      <th>Cde</th>\n",
       "      <th>Service hours type</th>\n",
       "      <th>Minor Repair / Tripping / Complication</th>\n",
       "      <th>Service hours type_lowercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RDRP</td>\n",
       "      <td>1A</td>\n",
       "      <td>RIG CARRIER MOVED TO NEW SITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rig carrier moved to new site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OUC</td>\n",
       "      <td>24J</td>\n",
       "      <td>DWS UNDER MAJOR REPAIR &amp; WAITG FOR COMP OF CIV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dws under major repair waitg for comp of civ job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Major code stored  as  workover/drilling  Cde  \\\n",
       "0                                     RDRP   1A   \n",
       "1                                      OUC  24J   \n",
       "2                                      OUC  24J   \n",
       "3                                      OUC  24J   \n",
       "4                                      OUC  24J   \n",
       "\n",
       "                                  Service hours type  \\\n",
       "0                      RIG CARRIER MOVED TO NEW SITE   \n",
       "1  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "2  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "3  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "4  DWS UNDER MAJOR REPAIR & WAITG FOR COMP OF CIV...   \n",
       "\n",
       "  Minor Repair / Tripping / Complication  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "                       Service hours type_lowercase  \n",
       "0                     rig carrier moved to new site  \n",
       "1  dws under major repair waitg for comp of civ job  \n",
       "2  dws under major repair waitg for comp of civ job  \n",
       "3  dws under major repair waitg for comp of civ job  \n",
       "4  dws under major repair waitg for comp of civ job  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d5e617-4fa7-4f20-9fbc-ed01b3d9469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_rare_samples(df, column_name, threshold=10):\n",
    "    # Get the frequency of each unique value in the specified column\n",
    "    value_counts = df[column_name].value_counts()\n",
    "\n",
    "    # Filter the DataFrame to retain only the rows where the count is greater than or equal to the threshold\n",
    "    fdf = df[df[column_name].isin(value_counts[value_counts >= threshold].index)]\n",
    "\n",
    "    return fdf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73dbc41-6c6f-4848-b699-b7632c51907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = discard_rare_samples(df,'Cde',threshold =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd5ff5e-9013-4092-a47f-ceeb05cb67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb984af-7d6b-429b-a9e2-59459d670347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Major code stored  as  workover/drilling', 'Cde', 'Service hours type',\n",
       "       'Minor Repair / Tripping / Complication',\n",
       "       'Service hours type_lowercase'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77b0bf5a-6909-4a97-a329-d754bca9baf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cde\n",
       "6A     67930\n",
       "5A     38754\n",
       "2A     22671\n",
       "23A    13612\n",
       "11A    13301\n",
       "       ...  \n",
       "23I       76\n",
       "7B        70\n",
       "11B       56\n",
       "24K       45\n",
       "21J       10\n",
       "Name: count, Length: 73, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf['Cde'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e860c04-10a3-477c-b9e8-c803fc29db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f515c0-7233-4a49-9660-9410276fbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2d15fc-9f13-473b-ab13-1442063a892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.loc[:, 'label_encoded'] = le.fit_transform(fdf['Cde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44780ca2-8587-4900-9b41-bee98a07600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Major code stored  as  workover/drilling', 'Cde', 'Service hours type',\n",
       "       'Minor Repair / Tripping / Complication',\n",
       "       'Service hours type_lowercase', 'label_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b4efa8-87fa-4550-b0d7-cc908c0b322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = fdf.drop(columns=['label_encoded'])  # Features\n",
    "y = fdf['label_encoded']  # Target\n",
    "\n",
    "# Perform stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,  # 30% test set\n",
    "    stratify=y,     # Ensure class distribution is preserved\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine features and target back into DataFrames\n",
    "train_df = X_train.copy()\n",
    "train_df['label_encoded'] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df['label_encoded'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "322224f9-4aaf-49d2-8973-fe63816bacce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_encoded\n",
      "65    0.233045\n",
      "64    0.132953\n",
      "59    0.077778\n",
      "41    0.046696\n",
      "1     0.045633\n",
      "        ...   \n",
      "49    0.000260\n",
      "67    0.000240\n",
      "2     0.000191\n",
      "58    0.000157\n",
      "29    0.000034\n",
      "Name: proportion, Length: 73, dtype: float64\n",
      "label_encoded\n",
      "65    0.233044\n",
      "64    0.132949\n",
      "59    0.077773\n",
      "41    0.046703\n",
      "1     0.045628\n",
      "        ...   \n",
      "49    0.000263\n",
      "67    0.000240\n",
      "2     0.000194\n",
      "58    0.000149\n",
      "29    0.000034\n",
      "Name: proportion, Length: 73, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify class distribution\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfac22fe-ecd8-42df-a30e-d946e834172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in d:\\hehe\\envs\\pytorch\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console contrib dejavu events execute kernel kernelspec\n",
      "lab labextension labhub migrate nbconvert nbextensions_configurator notebook\n",
      "qtconsole run script server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --upgrade\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f65858dc-baaf-498e-9ce9-2ea3e95a17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter-contrib-nbextensions\n",
      "  Downloading jupyter_contrib_nbextensions-0.7.0.tar.gz (23.5 MB)\n",
      "     ---------------------------------------- 0.0/23.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.8/23.5 MB 5.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 2.6/23.5 MB 7.6 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 3.9/23.5 MB 7.1 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 4.7/23.5 MB 6.1 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 5.2/23.5 MB 5.8 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 5.8/23.5 MB 5.3 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 6.3/23.5 MB 4.7 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 6.6/23.5 MB 4.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 7.1/23.5 MB 4.0 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 7.3/23.5 MB 3.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 7.9/23.5 MB 3.6 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 8.4/23.5 MB 3.5 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 8.7/23.5 MB 3.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 9.2/23.5 MB 3.2 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 9.4/23.5 MB 3.2 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 9.7/23.5 MB 3.1 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 10.2/23.5 MB 2.9 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 10.5/23.5 MB 2.9 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 10.7/23.5 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 11.0/23.5 MB 2.7 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 11.3/23.5 MB 2.7 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 11.5/23.5 MB 2.6 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 11.8/23.5 MB 2.5 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 11.8/23.5 MB 2.5 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.1/23.5 MB 2.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.1/23.5 MB 2.4 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 12.3/23.5 MB 2.3 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 12.6/23.5 MB 2.2 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 12.8/23.5 MB 2.2 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 13.1/23.5 MB 2.1 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 13.4/23.5 MB 2.1 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 13.6/23.5 MB 2.1 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 13.9/23.5 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------------------ --------------- 14.4/23.5 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 14.7/23.5 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 14.7/23.5 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 14.9/23.5 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 14.9/23.5 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 15.2/23.5 MB 1.9 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 15.5/23.5 MB 1.9 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 15.5/23.5 MB 1.9 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 15.7/23.5 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 15.7/23.5 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 16.0/23.5 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 16.3/23.5 MB 1.7 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 16.5/23.5 MB 1.7 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 16.8/23.5 MB 1.7 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 16.8/23.5 MB 1.7 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 17.0/23.5 MB 1.7 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 17.0/23.5 MB 1.7 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 17.3/23.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 17.3/23.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 17.6/23.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 17.6/23.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 17.8/23.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 17.8/23.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 18.1/23.5 MB 1.5 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 18.4/23.5 MB 1.5 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 18.6/23.5 MB 1.5 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 18.6/23.5 MB 1.5 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 18.9/23.5 MB 1.5 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 19.1/23.5 MB 1.5 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 19.1/23.5 MB 1.5 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 19.1/23.5 MB 1.5 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 19.4/23.5 MB 1.4 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 19.7/23.5 MB 1.4 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 19.9/23.5 MB 1.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 20.2/23.5 MB 1.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 20.2/23.5 MB 1.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 20.4/23.5 MB 1.4 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 20.7/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.0/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 21.2/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 21.2/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 21.5/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 21.5/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 21.8/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 21.8/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 21.8/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 21.8/23.5 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 22.0/23.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 22.0/23.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 22.0/23.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 22.0/23.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 22.0/23.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 22.3/23.5 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.3/23.5 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.5/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.5/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.5/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.5 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.1/23.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.3/23.5 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.3/23.5 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.3/23.5 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.3/23.5 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 23.5/23.5 MB 1.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ipython_genutils (from jupyter-contrib-nbextensions)\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting jupyter_contrib_core>=0.3.3 (from jupyter-contrib-nbextensions)\n",
      "  Downloading jupyter_contrib_core-0.4.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jupyter_core in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (5.7.2)\n",
      "Collecting jupyter_highlight_selected_word>=0.1.1 (from jupyter-contrib-nbextensions)\n",
      "  Downloading jupyter_highlight_selected_word-0.2.0-py2.py3-none-any.whl.metadata (730 bytes)\n",
      "Collecting jupyter_nbextensions_configurator>=0.4.0 (from jupyter-contrib-nbextensions)\n",
      "  Downloading jupyter_nbextensions_configurator-0.6.4-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nbconvert>=6.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (7.10.0)\n",
      "Requirement already satisfied: notebook>=6.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (7.0.8)\n",
      "Requirement already satisfied: tornado in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=4.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-contrib-nbextensions) (5.14.3)\n",
      "Collecting lxml (from jupyter-contrib-nbextensions)\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: setuptools in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter_contrib_core>=0.3.3->jupyter-contrib-nbextensions) (72.1.0)\n",
      "Requirement already satisfied: jupyter-server in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (2.14.1)\n",
      "Requirement already satisfied: pyyaml in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (3.1.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (5.9.2)\n",
      "Requirement already satisfied: packaging in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (24.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbconvert>=6.0->jupyter-contrib-nbextensions) (1.2.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter_core->jupyter-contrib-nbextensions) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter_core->jupyter-contrib-nbextensions) (305.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from notebook>=6.0->jupyter-contrib-nbextensions) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<4.1,>=4.0.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from notebook>=6.0->jupyter-contrib-nbextensions) (4.0.11)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from notebook>=6.0->jupyter-contrib-nbextensions) (0.2.3)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter-contrib-nbextensions) (1.16.0)\n",
      "Requirement already satisfied: webencodings in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter-contrib-nbextensions) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (21.3.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (8.6.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (0.4.4)\n",
      "Requirement already satisfied: overrides>=5.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (2.0.10)\n",
      "Requirement already satisfied: pyzmq>=24 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (6.28.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from nbformat>=5.7->nbconvert>=6.0->jupyter-contrib-nbextensions) (2.16.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.0->jupyter-contrib-nbextensions) (2.5)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2024.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (2.9.0.post0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter-contrib-nbextensions) (2024.8.30)\n",
      "Requirement already satisfied: comm>=0.1.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (8.25.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (1.6.0)\n",
      "Requirement already satisfied: psutil in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (5.9.0)\n",
      "Requirement already satisfied: decorator in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (3.0.43)\n",
      "Requirement already satisfied: stack-data in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.0)\n",
      "Requirement already satisfied: colorama in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.4.6)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions)\n",
      "  Downloading webcolors-24.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.5)\n",
      "Requirement already satisfied: arrow>=0.15.0 in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->jupyter_nbextensions_configurator>=0.4.0->jupyter-contrib-nbextensions) (1.2.3)\n",
      "Requirement already satisfied: executing in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\hehe\\envs\\pytorch\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=6.0->jupyter-contrib-nbextensions) (0.2.2)\n",
      "Downloading jupyter_highlight_selected_word-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading jupyter_nbextensions_configurator-0.6.4-py2.py3-none-any.whl (466 kB)\n",
      "Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 212.4 kB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 212.4 kB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 212.4 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 0.8/3.8 MB 258.1 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 0.8/3.8 MB 258.1 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 0.8/3.8 MB 258.1 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 305.1 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 305.1 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 1.3/3.8 MB 345.9 kB/s eta 0:00:08\n",
      "   ------------- -------------------------- 1.3/3.8 MB 345.9 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 388.4 kB/s eta 0:00:06\n",
      "   ------------------- -------------------- 1.8/3.8 MB 430.2 kB/s eta 0:00:05\n",
      "   --------------------- ------------------ 2.1/3.8 MB 471.7 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 2.1/3.8 MB 471.7 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 2.4/3.8 MB 500.9 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 2.4/3.8 MB 500.9 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 2.6/3.8 MB 500.1 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 2.6/3.8 MB 500.1 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 2.6/3.8 MB 500.1 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 2.6/3.8 MB 500.1 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 2.9/3.8 MB 474.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.9/3.8 MB 474.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.9/3.8 MB 474.0 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 3.1/3.8 MB 475.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 3.1/3.8 MB 475.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 3.1/3.8 MB 475.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 471.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 471.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 471.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 456.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 456.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 450.5 kB/s eta 0:00:00\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading webcolors-24.8.0-py3-none-any.whl (15 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: jupyter-contrib-nbextensions, jupyter_contrib_core\n",
      "  Building wheel for jupyter-contrib-nbextensions (setup.py): started\n",
      "  Building wheel for jupyter-contrib-nbextensions (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter-contrib-nbextensions: filename=jupyter_contrib_nbextensions-0.7.0-py2.py3-none-any.whl size=23428796 sha256=14bd34fd6e59ae9837eb3ee660e30331e2a06a4e5c8935ac693b6338f0ebebda\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\4d\\e4\\20\\dd16cdbbddcfe309a4559ea16adfa75fa5c554cfd076f9dae8\n",
      "  Building wheel for jupyter_contrib_core (setup.py): started\n",
      "  Building wheel for jupyter_contrib_core (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter_contrib_core: filename=jupyter_contrib_core-0.4.2-py2.py3-none-any.whl size=17492 sha256=50840f2c7131906064f62dc5a4e5a540c97444f07abdd08ff6484563e7955715\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\32\\f9\\af\\259a6f50f8b89d972aa81e31808a0af14e8bfc6867fb16ed07\n",
      "Successfully built jupyter-contrib-nbextensions jupyter_contrib_core\n",
      "Installing collected packages: jupyter_highlight_selected_word, ipython_genutils, webcolors, uri-template, lxml, jsonpointer, fqdn, isoduration, jupyter_contrib_core, jupyter_nbextensions_configurator, jupyter-contrib-nbextensions\n",
      "Successfully installed fqdn-1.5.1 ipython_genutils-0.2.0 isoduration-20.11.0 jsonpointer-3.0.0 jupyter-contrib-nbextensions-0.7.0 jupyter_contrib_core-0.4.2 jupyter_highlight_selected_word-0.2.0 jupyter_nbextensions_configurator-0.6.4 lxml-5.3.0 uri-template-1.3.0 webcolors-24.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter-contrib-nbextensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1658a548-409c-43fb-86f2-59525b129a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\nbextensions.py\", line 6, in <module>\n",
      "    from notebook.extensions import BaseExtensionApp\n",
      "ModuleNotFoundError: No module named 'notebook.extensions'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\nbextensions.py\", line 10, in <module>\n",
      "    from notebook.nbextensions import BaseNBExtensionApp\n",
      "ModuleNotFoundError: No module named 'notebook.nbextensions'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Scripts\\jupyter-contrib.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1073, in launch_instance\n",
      "    app = cls.instance(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\traitlets\\config\\configurable.py\", line 583, in instance\n",
      "    inst = cls(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\application.py\", line 27, in __init__\n",
      "    self._refresh_subcommands()\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\application.py\", line 43, in _refresh_subcommands\n",
      "    get_subcommands_dict = entrypoint.load()\n",
      "                           ^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2771, in load\n",
      "    return self.resolve()\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2777, in resolve\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_nbextensions\\application.py\", line 7, in <module>\n",
      "    from jupyter_contrib_core.notebook_compat.nbextensions import ArgumentConflict\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\nbextensions.py\", line 12, in <module>\n",
      "    from ._compat.nbextensions import BaseNBExtensionApp\n",
      "  File \"D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\jupyter_contrib_core\\notebook_compat\\_compat\\nbextensions.py\", line 35, in <module>\n",
      "    from notebook.nbextensions import (\n",
      "ModuleNotFoundError: No module named 'notebook.nbextensions'\n"
     ]
    }
   ],
   "source": [
    "!jupyter contrib nbextension install --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b97fa97-f833-4676-bdd8-0b3f36709fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2326, 2847, 2828, 1035, 2896, 18382, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokens = tokenizer('Service hours type_lowercase')\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2121992f-4c0e-4c11-9031-58e7eee950cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training data\n",
    "train_encodings = tokenizer(list(train_df['Service hours type_lowercase']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize the testing data\n",
    "test_encodings = tokenizer(list(test_df['Service hours type_lowercase']), truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24f19d44-33e2-43e5-9a3a-37edc5c7735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = CustomDataset(train_encodings, list(train_df['label_encoded']))\n",
    "test_dataset = CustomDataset(test_encodings, list(test_df['label_encoded']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d09cc42f-4c8e-45f8-b3ce-fe5edd7180dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Number of labels in your classification task\n",
    "num_labels = len(train_df['label_encoded'].unique())\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8ac12e3-ffbb-4239-b07a-b8eb1f748851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a49f97e7-16a4-4dea-a37e-6b9209f0a2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=6,              # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    evaluation_strategy='epoch',     # Evaluate after every epoch\n",
    "    save_strategy='epoch',           # Save model checkpoints after every epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56de6d5f-fcdf-4938-affd-47b7a4c74eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=test_dataset,           # Evaluation dataset\n",
    "    compute_metrics=compute_metrics     # Compute metrics function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5412978-1a14-4b87-9499-579f8b3aaf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38262' max='38262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38262/38262 1:29:41, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.703586</td>\n",
       "      <td>0.830572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.686266</td>\n",
       "      <td>0.836678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.717147</td>\n",
       "      <td>0.838416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.788059</td>\n",
       "      <td>0.837227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.851082</td>\n",
       "      <td>0.838565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.896517</td>\n",
       "      <td>0.837627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=38262, training_loss=0.344430530271553, metrics={'train_runtime': 5381.3567, 'train_samples_per_second': 227.499, 'train_steps_per_second': 7.11, 'total_flos': 9197237808264456.0, 'train_loss': 0.344430530271553, 'epoch': 6.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b7a0e46-ae0d-45c7-ab0f-3aad7c58c15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model\\\\tokenizer_config.json',\n",
       " './saved_model\\\\special_tokens_map.json',\n",
       " './saved_model\\\\vocab.txt',\n",
       " './saved_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c65c563-8016-4aa4-8905-2dd45383b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load the tokenizer from the saved directory\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "009a897e-0aa6-497d-bf7c-41fcb87330b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Define model path\n",
    "model_path = './saved_model'\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "957d67d2-9fba-4108-8cd0-cec46bb3e399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\hehe\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2733' max='2733' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2733/2733 02:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.8965165019035339, 'eval_accuracy': 0.8376273628597893, 'eval_runtime': 132.7013, 'eval_samples_per_second': 658.976, 'eval_steps_per_second': 20.595}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=test_dataset,           # Evaluation dataset\n",
    "    compute_metrics=compute_metrics      # Compute metrics function\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bd8df35-b540-4ce7-862c-73dc836ac73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"./saved_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ce108-b688-4544-8a46-06f310af375c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
